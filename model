digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2171395204224 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	2170780409744 [label=AddmmBackward0]
	2170780409792 -> 2170780409744
	2170779474560 [label="fc.bias
 (10)" fillcolor=lightblue]
	2170779474560 -> 2170780409792
	2170780409792 [label=AccumulateGrad]
	2170780407776 -> 2170780409744
	2170780407776 [label=ViewBackward0]
	2170780410416 -> 2170780407776
	2170780410416 [label=MeanBackward1]
	2170780409552 -> 2170780410416
	2170780409552 [label=ReluBackward0]
	2170780409408 -> 2170780409552
	2170780409408 [label=AddBackward0]
	2170780409360 -> 2170780409408
	2170780409360 [label=NativeBatchNormBackward0]
	2170780409168 -> 2170780409360
	2170780409168 [label=ConvolutionBackward0]
	2170780401872 -> 2170780409168
	2170780401872 [label=ReluBackward0]
	2170780404944 -> 2170780401872
	2170780404944 [label=NativeBatchNormBackward0]
	2170780405040 -> 2170780404944
	2170780405040 [label=ConvolutionBackward0]
	2170780404992 -> 2170780405040
	2170780404992 [label=ReluBackward0]
	2170780411616 -> 2170780404992
	2170780411616 [label=NativeBatchNormBackward0]
	2170780416416 -> 2170780411616
	2170780416416 [label=ConvolutionBackward0]
	2170780409648 -> 2170780416416
	2170780409648 [label=ReluBackward0]
	2170780405184 -> 2170780409648
	2170780405184 [label=AddBackward0]
	2170780411808 -> 2170780405184
	2170780411808 [label=NativeBatchNormBackward0]
	2170780413824 -> 2170780411808
	2170780413824 [label=ConvolutionBackward0]
	2170780412384 -> 2170780413824
	2170780412384 [label=ReluBackward0]
	2170780412192 -> 2170780412384
	2170780412192 [label=NativeBatchNormBackward0]
	2170780412048 -> 2170780412192
	2170780412048 [label=ConvolutionBackward0]
	2170780414544 -> 2170780412048
	2170780414544 [label=ReluBackward0]
	2170780414304 -> 2170780414544
	2170780414304 [label=NativeBatchNormBackward0]
	2170780414448 -> 2170780414304
	2170780414448 [label=ConvolutionBackward0]
	2170780414784 -> 2170780414448
	2170780414784 [label=ReluBackward0]
	2170780414736 -> 2170780414784
	2170780414736 [label=AddBackward0]
	2170780417088 -> 2170780414736
	2170780417088 [label=NativeBatchNormBackward0]
	2170780416080 -> 2170780417088
	2170780416080 [label=ConvolutionBackward0]
	2170780416800 -> 2170780416080
	2170780416800 [label=ReluBackward0]
	2170780416272 -> 2170780416800
	2170780416272 [label=NativeBatchNormBackward0]
	2170780410032 -> 2170780416272
	2170780410032 [label=ConvolutionBackward0]
	2170780409984 -> 2170780410032
	2170780409984 [label=ReluBackward0]
	2170780404800 -> 2170780409984
	2170780404800 [label=NativeBatchNormBackward0]
	2170780408112 -> 2170780404800
	2170780408112 [label=ConvolutionBackward0]
	2170780414064 -> 2170780408112
	2170780414064 [label=ReluBackward0]
	2171395133824 -> 2170780414064
	2171395133824 [label=AddBackward0]
	2171395134208 -> 2171395133824
	2171395134208 [label=NativeBatchNormBackward0]
	2170779057072 -> 2171395134208
	2170779057072 [label=ConvolutionBackward0]
	2170779208336 -> 2170779057072
	2170779208336 [label=ReluBackward0]
	2170779208192 -> 2170779208336
	2170779208192 [label=NativeBatchNormBackward0]
	2170779207088 -> 2170779208192
	2170779207088 [label=ConvolutionBackward0]
	2170181293424 -> 2170779207088
	2170181293424 [label=ReluBackward0]
	2170778933216 -> 2170181293424
	2170778933216 [label=NativeBatchNormBackward0]
	2170180928800 -> 2170778933216
	2170180928800 [label=ConvolutionBackward0]
	2171395133920 -> 2170180928800
	2171395133920 [label=ReluBackward0]
	2170495169728 -> 2171395133920
	2170495169728 [label=AddBackward0]
	2170769825920 -> 2170495169728
	2170769825920 [label=NativeBatchNormBackward0]
	2170769819632 -> 2170769825920
	2170769819632 [label=ConvolutionBackward0]
	2170769819344 -> 2170769819632
	2170769819344 [label=ReluBackward0]
	2170769833888 -> 2170769819344
	2170769833888 [label=NativeBatchNormBackward0]
	2170769830480 -> 2170769833888
	2170769830480 [label=ConvolutionBackward0]
	2171389763424 -> 2170769830480
	2171389763424 [label=ReluBackward0]
	2170777673616 -> 2171389763424
	2170777673616 [label=NativeBatchNormBackward0]
	2170777671984 -> 2170777673616
	2170777671984 [label=ConvolutionBackward0]
	2170769828272 -> 2170777671984
	2170769828272 [label=ReluBackward0]
	2170781547200 -> 2170769828272
	2170781547200 [label=AddBackward0]
	2171395455872 -> 2170781547200
	2171395455872 [label=NativeBatchNormBackward0]
	2171389752464 -> 2171395455872
	2171389752464 [label=ConvolutionBackward0]
	2171389752752 -> 2171389752464
	2171389752752 [label=ReluBackward0]
	2171389755248 -> 2171389752752
	2171389755248 [label=NativeBatchNormBackward0]
	2171389751936 -> 2171389755248
	2171389751936 [label=ConvolutionBackward0]
	2171389741328 -> 2171389751936
	2171389741328 [label=ReluBackward0]
	2171389743968 -> 2171389741328
	2171389743968 [label=NativeBatchNormBackward0]
	2171389746128 -> 2171389743968
	2171389746128 [label=ConvolutionBackward0]
	2171395452656 -> 2171389746128
	2171395452656 [label=ReluBackward0]
	2171389752848 -> 2171395452656
	2171389752848 [label=AddBackward0]
	2171389753184 -> 2171389752848
	2171389753184 [label=NativeBatchNormBackward0]
	2171389743056 -> 2171389753184
	2171389743056 [label=ConvolutionBackward0]
	2171395476400 -> 2171389743056
	2171395476400 [label=ReluBackward0]
	2171395478704 -> 2171395476400
	2171395478704 [label=NativeBatchNormBackward0]
	2171395476496 -> 2171395478704
	2171395476496 [label=ConvolutionBackward0]
	2171395476448 -> 2171395476496
	2171395476448 [label=ReluBackward0]
	2171395476832 -> 2171395476448
	2171395476832 [label=NativeBatchNormBackward0]
	2171395476592 -> 2171395476832
	2171395476592 [label=ConvolutionBackward0]
	2171389749920 -> 2171395476592
	2171389749920 [label=ReluBackward0]
	2171395477936 -> 2171389749920
	2171395477936 [label=AddBackward0]
	2171395477648 -> 2171395477936
	2171395477648 [label=NativeBatchNormBackward0]
	2171395476976 -> 2171395477648
	2171395476976 [label=ConvolutionBackward0]
	2171395478992 -> 2171395476976
	2171395478992 [label=ReluBackward0]
	2171395479136 -> 2171395478992
	2171395479136 [label=NativeBatchNormBackward0]
	2171395479232 -> 2171395479136
	2171395479232 [label=ConvolutionBackward0]
	2171395479424 -> 2171395479232
	2171395479424 [label=ReluBackward0]
	2171395479568 -> 2171395479424
	2171395479568 [label=NativeBatchNormBackward0]
	2171395479664 -> 2171395479568
	2171395479664 [label=ConvolutionBackward0]
	2171395477072 -> 2171395479664
	2171395477072 [label=ReluBackward0]
	2171395479952 -> 2171395477072
	2171395479952 [label=AddBackward0]
	2171395480048 -> 2171395479952
	2171395480048 [label=NativeBatchNormBackward0]
	2171395480192 -> 2171395480048
	2171395480192 [label=ConvolutionBackward0]
	2171395480384 -> 2171395480192
	2171395480384 [label=ReluBackward0]
	2171395480528 -> 2171395480384
	2171395480528 [label=NativeBatchNormBackward0]
	2171395480624 -> 2171395480528
	2171395480624 [label=ConvolutionBackward0]
	2171395480816 -> 2171395480624
	2171395480816 [label=ReluBackward0]
	2171395480960 -> 2171395480816
	2171395480960 [label=NativeBatchNormBackward0]
	2171395481056 -> 2171395480960
	2171395481056 [label=ConvolutionBackward0]
	2171395481248 -> 2171395481056
	2171395481248 [label=ReluBackward0]
	2171395481392 -> 2171395481248
	2171395481392 [label=AddBackward0]
	2171395481488 -> 2171395481392
	2171395481488 [label=NativeBatchNormBackward0]
	2171395481632 -> 2171395481488
	2171395481632 [label=ConvolutionBackward0]
	2171395481824 -> 2171395481632
	2171395481824 [label=ReluBackward0]
	2171395481968 -> 2171395481824
	2171395481968 [label=NativeBatchNormBackward0]
	2171395482064 -> 2171395481968
	2171395482064 [label=ConvolutionBackward0]
	2171395482256 -> 2171395482064
	2171395482256 [label=ReluBackward0]
	2171395482400 -> 2171395482256
	2171395482400 [label=NativeBatchNormBackward0]
	2171395482496 -> 2171395482400
	2171395482496 [label=ConvolutionBackward0]
	2171395481440 -> 2171395482496
	2171395481440 [label=ReluBackward0]
	2171395482784 -> 2171395481440
	2171395482784 [label=AddBackward0]
	2171395482880 -> 2171395482784
	2171395482880 [label=NativeBatchNormBackward0]
	2171395483024 -> 2171395482880
	2171395483024 [label=ConvolutionBackward0]
	2171395483216 -> 2171395483024
	2171395483216 [label=ReluBackward0]
	2171395483360 -> 2171395483216
	2171395483360 [label=NativeBatchNormBackward0]
	2171395483456 -> 2171395483360
	2171395483456 [label=ConvolutionBackward0]
	2171395483648 -> 2171395483456
	2171395483648 [label=ReluBackward0]
	2171395483792 -> 2171395483648
	2171395483792 [label=NativeBatchNormBackward0]
	2171395483888 -> 2171395483792
	2171395483888 [label=ConvolutionBackward0]
	2171395482832 -> 2171395483888
	2171395482832 [label=ReluBackward0]
	2171395484176 -> 2171395482832
	2171395484176 [label=AddBackward0]
	2171395484272 -> 2171395484176
	2171395484272 [label=NativeBatchNormBackward0]
	2171395484416 -> 2171395484272
	2171395484416 [label=ConvolutionBackward0]
	2171395484608 -> 2171395484416
	2171395484608 [label=ReluBackward0]
	2171395484752 -> 2171395484608
	2171395484752 [label=NativeBatchNormBackward0]
	2171395484848 -> 2171395484752
	2171395484848 [label=ConvolutionBackward0]
	2171395485040 -> 2171395484848
	2171395485040 [label=ReluBackward0]
	2171395485184 -> 2171395485040
	2171395485184 [label=NativeBatchNormBackward0]
	2171395485280 -> 2171395485184
	2171395485280 [label=ConvolutionBackward0]
	2171395484224 -> 2171395485280
	2171395484224 [label=ReluBackward0]
	2171395485568 -> 2171395484224
	2171395485568 [label=AddBackward0]
	2171395485664 -> 2171395485568
	2171395485664 [label=NativeBatchNormBackward0]
	2171395485808 -> 2171395485664
	2171395485808 [label=ConvolutionBackward0]
	2171395486000 -> 2171395485808
	2171395486000 [label=ReluBackward0]
	2171395486144 -> 2171395486000
	2171395486144 [label=NativeBatchNormBackward0]
	2171395486240 -> 2171395486144
	2171395486240 [label=ConvolutionBackward0]
	2171395486432 -> 2171395486240
	2171395486432 [label=ReluBackward0]
	2171395486576 -> 2171395486432
	2171395486576 [label=NativeBatchNormBackward0]
	2171395486672 -> 2171395486576
	2171395486672 [label=ConvolutionBackward0]
	2171395486864 -> 2171395486672
	2171395486864 [label=ReluBackward0]
	2171395487008 -> 2171395486864
	2171395487008 [label=AddBackward0]
	2171395487104 -> 2171395487008
	2171395487104 [label=NativeBatchNormBackward0]
	2171395487248 -> 2171395487104
	2171395487248 [label=ConvolutionBackward0]
	2171395487440 -> 2171395487248
	2171395487440 [label=ReluBackward0]
	2171395487584 -> 2171395487440
	2171395487584 [label=NativeBatchNormBackward0]
	2171395487680 -> 2171395487584
	2171395487680 [label=ConvolutionBackward0]
	2171395487872 -> 2171395487680
	2171395487872 [label=ReluBackward0]
	2171395488016 -> 2171395487872
	2171395488016 [label=NativeBatchNormBackward0]
	2171395488112 -> 2171395488016
	2171395488112 [label=ConvolutionBackward0]
	2171395487056 -> 2171395488112
	2171395487056 [label=ReluBackward0]
	2171395488400 -> 2171395487056
	2171395488400 [label=AddBackward0]
	2171395488496 -> 2171395488400
	2171395488496 [label=NativeBatchNormBackward0]
	2171395488640 -> 2171395488496
	2171395488640 [label=ConvolutionBackward0]
	2171395488832 -> 2171395488640
	2171395488832 [label=ReluBackward0]
	2171395488976 -> 2171395488832
	2171395488976 [label=NativeBatchNormBackward0]
	2171395489072 -> 2171395488976
	2171395489072 [label=ConvolutionBackward0]
	2171395489264 -> 2171395489072
	2171395489264 [label=ReluBackward0]
	2171395489408 -> 2171395489264
	2171395489408 [label=NativeBatchNormBackward0]
	2171395489504 -> 2171395489408
	2171395489504 [label=ConvolutionBackward0]
	2171395488448 -> 2171395489504
	2171395488448 [label=ReluBackward0]
	2171395489744 -> 2171395488448
	2171395489744 [label=AddBackward0]
	2171395539104 -> 2171395489744
	2171395539104 [label=NativeBatchNormBackward0]
	2171395539248 -> 2171395539104
	2171395539248 [label=ConvolutionBackward0]
	2171395539440 -> 2171395539248
	2171395539440 [label=ReluBackward0]
	2171395539584 -> 2171395539440
	2171395539584 [label=NativeBatchNormBackward0]
	2171395539680 -> 2171395539584
	2171395539680 [label=ConvolutionBackward0]
	2171395539872 -> 2171395539680
	2171395539872 [label=ReluBackward0]
	2171395540016 -> 2171395539872
	2171395540016 [label=NativeBatchNormBackward0]
	2171395540112 -> 2171395540016
	2171395540112 [label=ConvolutionBackward0]
	2171395540304 -> 2171395540112
	2171395540304 [label=MaxPool2DWithIndicesBackward0]
	2171395540448 -> 2171395540304
	2171395540448 [label=ReluBackward0]
	2171395540544 -> 2171395540448
	2171395540544 [label=NativeBatchNormBackward0]
	2171395540640 -> 2171395540544
	2171395540640 [label=ConvolutionBackward0]
	2171395540832 -> 2171395540640
	2170359550480 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2170359550480 -> 2171395540832
	2171395540832 [label=AccumulateGrad]
	2171395540592 -> 2171395540544
	2171172030720 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2171172030720 -> 2171395540592
	2171395540592 [label=AccumulateGrad]
	2171395540352 -> 2171395540544
	2171099927776 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2171099927776 -> 2171395540352
	2171395540352 [label=AccumulateGrad]
	2171395540256 -> 2171395540112
	2170495587632 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2170495587632 -> 2171395540256
	2171395540256 [label=AccumulateGrad]
	2171395540064 -> 2171395540016
	2170779680528 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2170779680528 -> 2171395540064
	2171395540064 [label=AccumulateGrad]
	2171395539920 -> 2171395540016
	2170779678208 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2170779678208 -> 2171395539920
	2171395539920 [label=AccumulateGrad]
	2171395539824 -> 2171395539680
	2170779677408 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2170779677408 -> 2171395539824
	2171395539824 [label=AccumulateGrad]
	2171395539632 -> 2171395539584
	2170779676928 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2170779676928 -> 2171395539632
	2171395539632 [label=AccumulateGrad]
	2171395539488 -> 2171395539584
	2170779677888 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2170779677888 -> 2171395539488
	2171395539488 [label=AccumulateGrad]
	2171395539392 -> 2171395539248
	2170779674208 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2170779674208 -> 2171395539392
	2171395539392 [label=AccumulateGrad]
	2171395539200 -> 2171395539104
	2170779674288 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2170779674288 -> 2171395539200
	2171395539200 [label=AccumulateGrad]
	2171395539152 -> 2171395539104
	2170779675328 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2170779675328 -> 2171395539152
	2171395539152 [label=AccumulateGrad]
	2171395539056 -> 2171395489744
	2171395539056 [label=NativeBatchNormBackward0]
	2171395539776 -> 2171395539056
	2171395539776 [label=ConvolutionBackward0]
	2171395540304 -> 2171395539776
	2171395540160 -> 2171395539776
	2170768001200 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2170768001200 -> 2171395540160
	2171395540160 [label=AccumulateGrad]
	2171395539344 -> 2171395539056
	2170779678368 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2170779678368 -> 2171395539344
	2171395539344 [label=AccumulateGrad]
	2171395539296 -> 2171395539056
	2170779679968 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2170779679968 -> 2171395539296
	2171395539296 [label=AccumulateGrad]
	2171395489696 -> 2171395489504
	2170779672368 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2170779672368 -> 2171395489696
	2171395489696 [label=AccumulateGrad]
	2171395489456 -> 2171395489408
	2170779674848 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2170779674848 -> 2171395489456
	2171395489456 [label=AccumulateGrad]
	2171395489312 -> 2171395489408
	2170779671968 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2170779671968 -> 2171395489312
	2171395489312 [label=AccumulateGrad]
	2171395489216 -> 2171395489072
	2170779665648 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2170779665648 -> 2171395489216
	2171395489216 [label=AccumulateGrad]
	2171395489024 -> 2171395488976
	2170779672048 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2170779672048 -> 2171395489024
	2171395489024 [label=AccumulateGrad]
	2171395488880 -> 2171395488976
	2170779671408 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2170779671408 -> 2171395488880
	2171395488880 [label=AccumulateGrad]
	2171395488784 -> 2171395488640
	2170779668688 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2170779668688 -> 2171395488784
	2171395488784 [label=AccumulateGrad]
	2171395488592 -> 2171395488496
	2170779664528 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2170779664528 -> 2171395488592
	2171395488592 [label=AccumulateGrad]
	2171395488544 -> 2171395488496
	2170779664448 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2170779664448 -> 2171395488544
	2171395488544 [label=AccumulateGrad]
	2171395488448 -> 2171395488400
	2171395488304 -> 2171395488112
	2170779667408 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2170779667408 -> 2171395488304
	2171395488304 [label=AccumulateGrad]
	2171395488064 -> 2171395488016
	2170779667968 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2170779667968 -> 2171395488064
	2171395488064 [label=AccumulateGrad]
	2171395487920 -> 2171395488016
	2170779667088 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2170779667088 -> 2171395487920
	2171395487920 [label=AccumulateGrad]
	2171395487824 -> 2171395487680
	2170779675728 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2170779675728 -> 2171395487824
	2171395487824 [label=AccumulateGrad]
	2171395487632 -> 2171395487584
	2171284211568 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2171284211568 -> 2171395487632
	2171395487632 [label=AccumulateGrad]
	2171395487488 -> 2171395487584
	2171284214208 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2171284214208 -> 2171395487488
	2171395487488 [label=AccumulateGrad]
	2171395487392 -> 2171395487248
	2171284218928 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2171284218928 -> 2171395487392
	2171395487392 [label=AccumulateGrad]
	2171395487200 -> 2171395487104
	2171284210848 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2171284210848 -> 2171395487200
	2171395487200 [label=AccumulateGrad]
	2171395487152 -> 2171395487104
	2171284211328 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2171284211328 -> 2171395487152
	2171395487152 [label=AccumulateGrad]
	2171395487056 -> 2171395487008
	2171395486816 -> 2171395486672
	2171284216128 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2171284216128 -> 2171395486816
	2171395486816 [label=AccumulateGrad]
	2171395486624 -> 2171395486576
	2171284213808 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2171284213808 -> 2171395486624
	2171395486624 [label=AccumulateGrad]
	2171395486480 -> 2171395486576
	2171284215328 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2171284215328 -> 2171395486480
	2171395486480 [label=AccumulateGrad]
	2171395486384 -> 2171395486240
	2171284212368 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2171284212368 -> 2171395486384
	2171395486384 [label=AccumulateGrad]
	2171395486192 -> 2171395486144
	2171284225248 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2171284225248 -> 2171395486192
	2171395486192 [label=AccumulateGrad]
	2171395486048 -> 2171395486144
	2171284222208 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2171284222208 -> 2171395486048
	2171395486048 [label=AccumulateGrad]
	2171395485952 -> 2171395485808
	2171284211408 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2171284211408 -> 2171395485952
	2171395485952 [label=AccumulateGrad]
	2171395485760 -> 2171395485664
	2171284217248 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2171284217248 -> 2171395485760
	2171395485760 [label=AccumulateGrad]
	2171395485712 -> 2171395485664
	2171284223568 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2171284223568 -> 2171395485712
	2171395485712 [label=AccumulateGrad]
	2171395485616 -> 2171395485568
	2171395485616 [label=NativeBatchNormBackward0]
	2171395486336 -> 2171395485616
	2171395486336 [label=ConvolutionBackward0]
	2171395486864 -> 2171395486336
	2171395486720 -> 2171395486336
	2171284213408 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2171284213408 -> 2171395486720
	2171395486720 [label=AccumulateGrad]
	2171395485904 -> 2171395485616
	2171284219648 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2171284219648 -> 2171395485904
	2171395485904 [label=AccumulateGrad]
	2171395485856 -> 2171395485616
	2171284217888 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2171284217888 -> 2171395485856
	2171395485856 [label=AccumulateGrad]
	2171395485472 -> 2171395485280
	2170779606912 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2170779606912 -> 2171395485472
	2171395485472 [label=AccumulateGrad]
	2171395485232 -> 2171395485184
	2170779608752 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2170779608752 -> 2171395485232
	2171395485232 [label=AccumulateGrad]
	2171395485088 -> 2171395485184
	2170779611072 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2170779611072 -> 2171395485088
	2171395485088 [label=AccumulateGrad]
	2171395484992 -> 2171395484848
	2170779612192 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2170779612192 -> 2171395484992
	2171395484992 [label=AccumulateGrad]
	2171395484800 -> 2171395484752
	2170779612112 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2170779612112 -> 2171395484800
	2171395484800 [label=AccumulateGrad]
	2171395484656 -> 2171395484752
	2170779614272 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2170779614272 -> 2171395484656
	2171395484656 [label=AccumulateGrad]
	2171395484560 -> 2171395484416
	2170779614592 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2170779614592 -> 2171395484560
	2171395484560 [label=AccumulateGrad]
	2171395484368 -> 2171395484272
	2170779614832 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2170779614832 -> 2171395484368
	2171395484368 [label=AccumulateGrad]
	2171395484320 -> 2171395484272
	2170779615072 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2170779615072 -> 2171395484320
	2171395484320 [label=AccumulateGrad]
	2171395484224 -> 2171395484176
	2171395484080 -> 2171395483888
	2170779613552 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2170779613552 -> 2171395484080
	2171395484080 [label=AccumulateGrad]
	2171395483840 -> 2171395483792
	2170779613632 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2170779613632 -> 2171395483840
	2171395483840 [label=AccumulateGrad]
	2171395483696 -> 2171395483792
	2170779613872 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2170779613872 -> 2171395483696
	2171395483696 [label=AccumulateGrad]
	2171395483600 -> 2171395483456
	2170779613312 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2170779613312 -> 2171395483600
	2171395483600 [label=AccumulateGrad]
	2171395483408 -> 2171395483360
	2170779613712 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2170779613712 -> 2171395483408
	2171395483408 [label=AccumulateGrad]
	2171395483264 -> 2171395483360
	2170779612352 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2170779612352 -> 2171395483264
	2171395483264 [label=AccumulateGrad]
	2171395483168 -> 2171395483024
	2170779611152 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2170779611152 -> 2171395483168
	2171395483168 [label=AccumulateGrad]
	2171395482976 -> 2171395482880
	2170779611232 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2170779611232 -> 2171395482976
	2171395482976 [label=AccumulateGrad]
	2171395482928 -> 2171395482880
	2170779608992 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2170779608992 -> 2171395482928
	2171395482928 [label=AccumulateGrad]
	2171395482832 -> 2171395482784
	2171395482688 -> 2171395482496
	2170779610992 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2170779610992 -> 2171395482688
	2171395482688 [label=AccumulateGrad]
	2171395482448 -> 2171395482400
	2170779608912 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2170779608912 -> 2171395482448
	2171395482448 [label=AccumulateGrad]
	2171395482304 -> 2171395482400
	2170779609872 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2170779609872 -> 2171395482304
	2171395482304 [label=AccumulateGrad]
	2171395482208 -> 2171395482064
	2170779607392 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2170779607392 -> 2171395482208
	2171395482208 [label=AccumulateGrad]
	2171395482016 -> 2171395481968
	2170779607552 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2170779607552 -> 2171395482016
	2171395482016 [label=AccumulateGrad]
	2171395481872 -> 2171395481968
	2170779607472 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2170779607472 -> 2171395481872
	2171395481872 [label=AccumulateGrad]
	2171395481776 -> 2171395481632
	2170779605472 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2170779605472 -> 2171395481776
	2171395481776 [label=AccumulateGrad]
	2171395481584 -> 2171395481488
	2170779605632 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2170779605632 -> 2171395481584
	2171395481584 [label=AccumulateGrad]
	2171395481536 -> 2171395481488
	2170779606672 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2170779606672 -> 2171395481536
	2171395481536 [label=AccumulateGrad]
	2171395481440 -> 2171395481392
	2171395481200 -> 2171395481056
	2170779601632 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2170779601632 -> 2171395481200
	2171395481200 [label=AccumulateGrad]
	2171395481008 -> 2171395480960
	2170779602272 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2170779602272 -> 2171395481008
	2171395481008 [label=AccumulateGrad]
	2171395480864 -> 2171395480960
	2170779602032 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2170779602032 -> 2171395480864
	2171395480864 [label=AccumulateGrad]
	2171395480768 -> 2171395480624
	2170779600592 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779600592 -> 2171395480768
	2171395480768 [label=AccumulateGrad]
	2171395480576 -> 2171395480528
	2170779601552 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2170779601552 -> 2171395480576
	2171395480576 [label=AccumulateGrad]
	2171395480432 -> 2171395480528
	2170779601232 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2170779601232 -> 2171395480432
	2171395480432 [label=AccumulateGrad]
	2171395480336 -> 2171395480192
	2170779560880 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2170779560880 -> 2171395480336
	2171395480336 [label=AccumulateGrad]
	2171395480144 -> 2171395480048
	2170779560640 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2170779560640 -> 2171395480144
	2171395480144 [label=AccumulateGrad]
	2171395480096 -> 2171395480048
	2170779561120 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2170779561120 -> 2171395480096
	2171395480096 [label=AccumulateGrad]
	2171395480000 -> 2171395479952
	2171395480000 [label=NativeBatchNormBackward0]
	2171395480720 -> 2171395480000
	2171395480720 [label=ConvolutionBackward0]
	2171395481248 -> 2171395480720
	2171395481104 -> 2171395480720
	2170779605552 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2170779605552 -> 2171395481104
	2171395481104 [label=AccumulateGrad]
	2171395480288 -> 2171395480000
	2170779605392 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2170779605392 -> 2171395480288
	2171395480288 [label=AccumulateGrad]
	2171395480240 -> 2171395480000
	2170779605072 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2170779605072 -> 2171395480240
	2171395480240 [label=AccumulateGrad]
	2171395479856 -> 2171395479664
	2170779561920 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2170779561920 -> 2171395479856
	2171395479856 [label=AccumulateGrad]
	2171395479616 -> 2171395479568
	2170779562000 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2170779562000 -> 2171395479616
	2171395479616 [label=AccumulateGrad]
	2171395479472 -> 2171395479568
	2170779561840 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2170779561840 -> 2171395479472
	2171395479472 [label=AccumulateGrad]
	2171395479376 -> 2171395479232
	2170779564400 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779564400 -> 2171395479376
	2171395479376 [label=AccumulateGrad]
	2171395479184 -> 2171395479136
	2170779564720 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2170779564720 -> 2171395479184
	2171395479184 [label=AccumulateGrad]
	2171395479040 -> 2171395479136
	2170779564880 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2170779564880 -> 2171395479040
	2171395479040 [label=AccumulateGrad]
	2171395478944 -> 2171395476976
	2170779565040 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2170779565040 -> 2171395478944
	2171395478944 [label=AccumulateGrad]
	2171395477792 -> 2171395477648
	2170779563760 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2170779563760 -> 2171395477792
	2171395477792 [label=AccumulateGrad]
	2171395476640 -> 2171395477648
	2170779565920 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2170779565920 -> 2171395476640
	2171395476640 [label=AccumulateGrad]
	2171395477072 -> 2171395477936
	2171395478080 -> 2171395476592
	2170779563200 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2170779563200 -> 2171395478080
	2171395478080 [label=AccumulateGrad]
	2171395476784 -> 2171395476832
	2170779563280 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2170779563280 -> 2171395476784
	2171395476784 [label=AccumulateGrad]
	2171395476688 -> 2171395476832
	2170779563600 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2170779563600 -> 2171395476688
	2171395476688 [label=AccumulateGrad]
	2171395475776 -> 2171395476496
	2170779562720 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779562720 -> 2171395475776
	2171395475776 [label=AccumulateGrad]
	2171395478800 -> 2171395478704
	2170779562880 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2170779562880 -> 2171395478800
	2171395478800 [label=AccumulateGrad]
	2171395476064 -> 2171395478704
	2170779561360 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2170779561360 -> 2171395476064
	2171395476064 [label=AccumulateGrad]
	2171395476208 -> 2171389743056
	2170779560960 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2170779560960 -> 2171395476208
	2171395476208 [label=AccumulateGrad]
	2171389749296 -> 2171389753184
	2170779560400 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2170779560400 -> 2171389749296
	2171389749296 [label=AccumulateGrad]
	2171395475488 -> 2171389753184
	2170779560320 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2170779560320 -> 2171395475488
	2171395475488 [label=AccumulateGrad]
	2171389749920 -> 2171389752848
	2171389748288 -> 2171389746128
	2170779558160 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2170779558160 -> 2171389748288
	2171389748288 [label=AccumulateGrad]
	2171389748624 -> 2171389743968
	2170779560080 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2170779560080 -> 2171389748624
	2171389748624 [label=AccumulateGrad]
	2171389753424 -> 2171389743968
	2170779557840 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2170779557840 -> 2171389753424
	2171389753424 [label=AccumulateGrad]
	2171389742000 -> 2171389751936
	2170779557200 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779557200 -> 2171389742000
	2171389742000 [label=AccumulateGrad]
	2171389742576 -> 2171389755248
	2170779557120 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2170779557120 -> 2171389742576
	2171389742576 [label=AccumulateGrad]
	2171389742960 -> 2171389755248
	2170779557360 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2170779557360 -> 2171389742960
	2171389742960 [label=AccumulateGrad]
	2171389747856 -> 2171389752464
	2170779555200 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2170779555200 -> 2171389747856
	2171389747856 [label=AccumulateGrad]
	2171389742432 -> 2171395455872
	2170779556240 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2170779556240 -> 2171389742432
	2171389742432 [label=AccumulateGrad]
	2171389755056 -> 2171395455872
	2170779555280 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2170779555280 -> 2171389755056
	2171389755056 [label=AccumulateGrad]
	2171395452656 -> 2170781547200
	2171389648448 -> 2170777671984
	2170779554960 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2170779554960 -> 2171389648448
	2171389648448 [label=AccumulateGrad]
	2170777673760 -> 2170777673616
	2170779554640 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2170779554640 -> 2170777673760
	2170777673760 [label=AccumulateGrad]
	2170777673136 -> 2170777673616
	2170779554480 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2170779554480 -> 2170777673136
	2170777673136 [label=AccumulateGrad]
	2170769833648 -> 2170769830480
	2170779551520 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779551520 -> 2170769833648
	2170769833648 [label=AccumulateGrad]
	2170769820928 -> 2170769833888
	2170779550880 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2170779550880 -> 2170769820928
	2170769820928 [label=AccumulateGrad]
	2170769827312 -> 2170769833888
	2170779550400 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2170779550400 -> 2170769827312
	2170769827312 [label=AccumulateGrad]
	2170769821360 -> 2170769819632
	2171547285152 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2171547285152 -> 2170769821360
	2170769821360 [label=AccumulateGrad]
	2170769831392 -> 2170769825920
	2171547285872 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2171547285872 -> 2170769831392
	2170769831392 [label=AccumulateGrad]
	2170769823520 -> 2170769825920
	2170768080496 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2170768080496 -> 2170769823520
	2170769823520 [label=AccumulateGrad]
	2170769828272 -> 2170495169728
	2170495171216 -> 2170180928800
	2170779014128 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2170779014128 -> 2170495171216
	2170495171216 [label=AccumulateGrad]
	2170778932400 -> 2170778933216
	2170779020128 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2170779020128 -> 2170778932400
	2170778932400 [label=AccumulateGrad]
	2170778931152 -> 2170778933216
	2170779020288 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2170779020288 -> 2170778931152
	2170778931152 [label=AccumulateGrad]
	2170779208720 -> 2170779207088
	2170779021808 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2170779021808 -> 2170779208720
	2170779208720 [label=AccumulateGrad]
	2170779207904 -> 2170779208192
	2170779020848 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2170779020848 -> 2170779207904
	2170779207904 [label=AccumulateGrad]
	2170779206080 -> 2170779208192
	2170779021568 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2170779021568 -> 2170779206080
	2170779206080 [label=AccumulateGrad]
	2170779209536 -> 2170779057072
	2170779013728 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2170779013728 -> 2170779209536
	2170779209536 [label=AccumulateGrad]
	2170779215344 -> 2171395134208
	2170779024208 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2170779024208 -> 2170779215344
	2170779215344 [label=AccumulateGrad]
	2170779212608 -> 2171395134208
	2170779014848 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2170779014848 -> 2170779212608
	2170779212608 [label=AccumulateGrad]
	2171395133920 -> 2171395133824
	2170780413872 -> 2170780408112
	2170779478320 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2170779478320 -> 2170780413872
	2170780413872 [label=AccumulateGrad]
	2170780402928 -> 2170780404800
	2170779478480 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2170779478480 -> 2170780402928
	2170780402928 [label=AccumulateGrad]
	2170780404896 -> 2170780404800
	2170779478400 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2170779478400 -> 2170780404896
	2170780404896 [label=AccumulateGrad]
	2170780410176 -> 2170780410032
	2170779479760 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2170779479760 -> 2170780410176
	2170780410176 [label=AccumulateGrad]
	2170780416656 -> 2170780416272
	2170779480320 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2170779480320 -> 2170780416656
	2170780416656 [label=AccumulateGrad]
	2170780416464 -> 2170780416272
	2170779480400 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2170779480400 -> 2170780416464
	2170780416464 [label=AccumulateGrad]
	2170780416128 -> 2170780416080
	2170779481280 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2170779481280 -> 2170780416128
	2170780416128 [label=AccumulateGrad]
	2170780416512 -> 2170780417088
	2170779481120 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2170779481120 -> 2170780416512
	2170780416512 [label=AccumulateGrad]
	2170780416848 -> 2170780417088
	2170779481360 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2170779481360 -> 2170780416848
	2170780416848 [label=AccumulateGrad]
	2170780415024 -> 2170780414736
	2170780415024 [label=NativeBatchNormBackward0]
	2170780410128 -> 2170780415024
	2170780410128 [label=ConvolutionBackward0]
	2170780414064 -> 2170780410128
	2170780410992 -> 2170780410128
	2170779012848 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2170779012848 -> 2170780410992
	2170780410992 [label=AccumulateGrad]
	2170780416320 -> 2170780415024
	2170779482960 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2170779482960 -> 2170780416320
	2170780416320 [label=AccumulateGrad]
	2170780416224 -> 2170780415024
	2170779474960 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2170779474960 -> 2170780416224
	2170780416224 [label=AccumulateGrad]
	2170780414496 -> 2170780414448
	2170779481920 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2170779481920 -> 2170780414496
	2170780414496 [label=AccumulateGrad]
	2170780414352 -> 2170780414304
	2170779482000 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2170779482000 -> 2170780414352
	2170780414352 [label=AccumulateGrad]
	2170780414592 -> 2170780414304
	2170779482240 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2170779482240 -> 2170780414592
	2170780414592 [label=AccumulateGrad]
	2170780414688 -> 2170780412048
	2170779484080 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2170779484080 -> 2170780414688
	2170780414688 [label=AccumulateGrad]
	2170780412000 -> 2170780412192
	2170779478160 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2170779478160 -> 2170780412000
	2170780412000 [label=AccumulateGrad]
	2170780412144 -> 2170780412192
	2170779480560 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2170779480560 -> 2170780412144
	2170780412144 [label=AccumulateGrad]
	2170780412336 -> 2170780413824
	2170779480080 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2170779480080 -> 2170780412336
	2170780412336 [label=AccumulateGrad]
	2170780411952 -> 2170780411808
	2170779480480 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2170779480480 -> 2170780411952
	2170780411952 [label=AccumulateGrad]
	2170780411856 -> 2170780411808
	2170779478960 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2170779478960 -> 2170780411856
	2170780411856 [label=AccumulateGrad]
	2170780414784 -> 2170780405184
	2170780411472 -> 2170780416416
	2170779480240 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2170779480240 -> 2170780411472
	2170780411472 [label=AccumulateGrad]
	2170780411376 -> 2170780411616
	2170779479680 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2170779479680 -> 2170780411376
	2170780411376 [label=AccumulateGrad]
	2170780407728 -> 2170780411616
	2170779479600 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2170779479600 -> 2170780407728
	2170780407728 [label=AccumulateGrad]
	2170780404752 -> 2170780405040
	2170779477920 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2170779477920 -> 2170780404752
	2170780404752 [label=AccumulateGrad]
	2170780405280 -> 2170780404944
	2170779477120 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2170779477120 -> 2170780405280
	2170780405280 [label=AccumulateGrad]
	2170780403312 -> 2170780404944
	2170779477200 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2170779477200 -> 2170780403312
	2170780403312 [label=AccumulateGrad]
	2170780409216 -> 2170780409168
	2170779475680 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2170779475680 -> 2170780409216
	2170780409216 [label=AccumulateGrad]
	2170780408976 -> 2170780409360
	2170779475520 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2170779475520 -> 2170780408976
	2170780408976 [label=AccumulateGrad]
	2170780409024 -> 2170780409360
	2170779476480 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2170779476480 -> 2170780409024
	2170780409024 [label=AccumulateGrad]
	2170780409648 -> 2170780409408
	2170780410656 -> 2170780409744
	2170780410656 [label=TBackward0]
	2170780409504 -> 2170780410656
	2170779476720 [label="fc.weight
 (10, 2048)" fillcolor=lightblue]
	2170779476720 -> 2170780409504
	2170780409504 [label=AccumulateGrad]
	2170780409744 -> 2171395204224
}
