digraph {
	graph [size="151.95,151.95"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	2248759602016 [label="
 (1, 10)" fillcolor=darkolivegreen1]
	2250504406592 [label=AddmmBackward0]
	2250504411632 -> 2250504406592
	2250506615152 [label="fc.bias
 (10)" fillcolor=lightblue]
	2250506615152 -> 2250504411632
	2250504411632 [label=AccumulateGrad]
	2250504415808 -> 2250504406592
	2250504415808 [label=ViewBackward0]
	2250504420080 -> 2250504415808
	2250504420080 [label=MeanBackward1]
	2250504406784 -> 2250504420080
	2250504406784 [label=ReluBackward0]
	2250504406736 -> 2250504406784
	2250504406736 [label=AddBackward0]
	2250504409376 -> 2250504406736
	2250504409376 [label=NativeBatchNormBackward0]
	2250504408464 -> 2250504409376
	2250504408464 [label=ConvolutionBackward0]
	2250504413120 -> 2250504408464
	2250504413120 [label=ReluBackward0]
	2250504417872 -> 2250504413120
	2250504417872 [label=NativeBatchNormBackward0]
	2250504418592 -> 2250504417872
	2250504418592 [label=ConvolutionBackward0]
	2250504411680 -> 2250504418592
	2250504411680 [label=ReluBackward0]
	2250504414560 -> 2250504411680
	2250504414560 [label=NativeBatchNormBackward0]
	2250504419312 -> 2250504414560
	2250504419312 [label=ConvolutionBackward0]
	2250504410000 -> 2250504419312
	2250504410000 [label=ReluBackward0]
	2250504416624 -> 2250504410000
	2250504416624 [label=AddBackward0]
	2250504415952 -> 2250504416624
	2250504415952 [label=NativeBatchNormBackward0]
	2250504409472 -> 2250504415952
	2250504409472 [label=ConvolutionBackward0]
	2250504409232 -> 2250504409472
	2250504409232 [label=ReluBackward0]
	2250504420800 -> 2250504409232
	2250504420800 [label=NativeBatchNormBackward0]
	2250504411296 -> 2250504420800
	2250504411296 [label=ConvolutionBackward0]
	2250504420368 -> 2250504411296
	2250504420368 [label=ReluBackward0]
	2250504412304 -> 2250504420368
	2250504412304 [label=NativeBatchNormBackward0]
	2250504412352 -> 2250504412304
	2250504412352 [label=ConvolutionBackward0]
	2250504415616 -> 2250504412352
	2250504415616 [label=ReluBackward0]
	2250504409520 -> 2250504415616
	2250504409520 [label=AddBackward0]
	2250504405248 -> 2250504409520
	2250504405248 [label=NativeBatchNormBackward0]
	2250504413168 -> 2250504405248
	2250504413168 [label=ConvolutionBackward0]
	2250504411872 -> 2250504413168
	2250504411872 [label=ReluBackward0]
	2250504419984 -> 2250504411872
	2250504419984 [label=NativeBatchNormBackward0]
	2250504408128 -> 2250504419984
	2250504408128 [label=ConvolutionBackward0]
	2250504405824 -> 2250504408128
	2250504405824 [label=ReluBackward0]
	2250504418016 -> 2250504405824
	2250504418016 [label=NativeBatchNormBackward0]
	2250504416720 -> 2250504418016
	2250504416720 [label=ConvolutionBackward0]
	2250504405200 -> 2250504416720
	2250504405200 [label=ReluBackward0]
	2250504413216 -> 2250504405200
	2250504413216 [label=AddBackward0]
	2250504414608 -> 2250504413216
	2250504414608 [label=NativeBatchNormBackward0]
	2250504407984 -> 2250504414608
	2250504407984 [label=ConvolutionBackward0]
	2250504410096 -> 2250504407984
	2250504410096 [label=ReluBackward0]
	2250504411824 -> 2250504410096
	2250504411824 [label=NativeBatchNormBackward0]
	2250504417392 -> 2250504411824
	2250504417392 [label=ConvolutionBackward0]
	2250504411968 -> 2250504417392
	2250504411968 [label=ReluBackward0]
	2250504407456 -> 2250504411968
	2250504407456 [label=NativeBatchNormBackward0]
	2250504414080 -> 2250504407456
	2250504414080 [label=ConvolutionBackward0]
	2250504417344 -> 2250504414080
	2250504417344 [label=ReluBackward0]
	2250504413552 -> 2250504417344
	2250504413552 [label=AddBackward0]
	2250504413456 -> 2250504413552
	2250504413456 [label=NativeBatchNormBackward0]
	2250504413840 -> 2250504413456
	2250504413840 [label=ConvolutionBackward0]
	2250504413984 -> 2250504413840
	2250504413984 [label=ReluBackward0]
	2250504418256 -> 2250504413984
	2250504418256 [label=NativeBatchNormBackward0]
	2250504418400 -> 2250504418256
	2250504418400 [label=ConvolutionBackward0]
	2250504418544 -> 2250504418400
	2250504418544 [label=ReluBackward0]
	2250504415520 -> 2250504418544
	2250504415520 [label=NativeBatchNormBackward0]
	2250504409280 -> 2250504415520
	2250504409280 [label=ConvolutionBackward0]
	2250504413504 -> 2250504409280
	2250504413504 [label=ReluBackward0]
	2250504413264 -> 2250504413504
	2250504413264 [label=AddBackward0]
	2250504410384 -> 2250504413264
	2250504410384 [label=NativeBatchNormBackward0]
	2250504410240 -> 2250504410384
	2250504410240 [label=ConvolutionBackward0]
	2250504419120 -> 2250504410240
	2250504419120 [label=ReluBackward0]
	2250504405488 -> 2250504419120
	2250504405488 [label=NativeBatchNormBackward0]
	2250504408752 -> 2250504405488
	2250504408752 [label=ConvolutionBackward0]
	2250504408896 -> 2250504408752
	2250504408896 [label=ReluBackward0]
	2250504408704 -> 2250504408896
	2250504408704 [label=NativeBatchNormBackward0]
	2250504419456 -> 2250504408704
	2250504419456 [label=ConvolutionBackward0]
	2250504413312 -> 2250504419456
	2250504413312 [label=ReluBackward0]
	2250504405296 -> 2250504413312
	2250504405296 [label=AddBackward0]
	2250504414224 -> 2250504405296
	2250504414224 [label=NativeBatchNormBackward0]
	2250504414464 -> 2250504414224
	2250504414464 [label=ConvolutionBackward0]
	2250504483168 -> 2250504414464
	2250504483168 [label=ReluBackward0]
	2250504480528 -> 2250504483168
	2250504480528 [label=NativeBatchNormBackward0]
	2250504480096 -> 2250504480528
	2250504480096 [label=ConvolutionBackward0]
	2250504486288 -> 2250504480096
	2250504486288 [label=ReluBackward0]
	2250504476016 -> 2250504486288
	2250504476016 [label=NativeBatchNormBackward0]
	2250504480480 -> 2250504476016
	2250504480480 [label=ConvolutionBackward0]
	2250504414272 -> 2250504480480
	2250504414272 [label=ReluBackward0]
	2250504485520 -> 2250504414272
	2250504485520 [label=AddBackward0]
	2250504481680 -> 2250504485520
	2250504481680 [label=NativeBatchNormBackward0]
	2250504475392 -> 2250504481680
	2250504475392 [label=ConvolutionBackward0]
	2250504475056 -> 2250504475392
	2250504475056 [label=ReluBackward0]
	2250504480048 -> 2250504475056
	2250504480048 [label=NativeBatchNormBackward0]
	2250504480288 -> 2250504480048
	2250504480288 [label=ConvolutionBackward0]
	2250504479424 -> 2250504480288
	2250504479424 [label=ReluBackward0]
	2250504470880 -> 2250504479424
	2250504470880 [label=NativeBatchNormBackward0]
	2250504481344 -> 2250504470880
	2250504481344 [label=ConvolutionBackward0]
	2250504480336 -> 2250504481344
	2250504480336 [label=ReluBackward0]
	2250504479808 -> 2250504480336
	2250504479808 [label=AddBackward0]
	2250504479904 -> 2250504479808
	2250504479904 [label=NativeBatchNormBackward0]
	2250504480816 -> 2250504479904
	2250504480816 [label=ConvolutionBackward0]
	2250504540640 -> 2250504480816
	2250504540640 [label=ReluBackward0]
	2250504542032 -> 2250504540640
	2250504542032 [label=NativeBatchNormBackward0]
	2250504550336 -> 2250504542032
	2250504550336 [label=ConvolutionBackward0]
	2250504550528 -> 2250504550336
	2250504550528 [label=ReluBackward0]
	2250504550624 -> 2250504550528
	2250504550624 [label=NativeBatchNormBackward0]
	2250504550240 -> 2250504550624
	2250504550240 [label=ConvolutionBackward0]
	2250504550720 -> 2250504550240
	2250504550720 [label=ReluBackward0]
	2250504549952 -> 2250504550720
	2250504549952 [label=AddBackward0]
	2250504549808 -> 2250504549952
	2250504549808 [label=NativeBatchNormBackward0]
	2250504550864 -> 2250504549808
	2250504550864 [label=ConvolutionBackward0]
	2250504542944 -> 2250504550864
	2250504542944 [label=ReluBackward0]
	2250504549040 -> 2250504542944
	2250504549040 [label=NativeBatchNormBackward0]
	2250504549136 -> 2250504549040
	2250504549136 [label=ConvolutionBackward0]
	2250504551920 -> 2250504549136
	2250504551920 [label=ReluBackward0]
	2250504552064 -> 2250504551920
	2250504552064 [label=NativeBatchNormBackward0]
	2250504548944 -> 2250504552064
	2250504548944 [label=ConvolutionBackward0]
	2250504550816 -> 2250504548944
	2250504550816 [label=ReluBackward0]
	2250504548608 -> 2250504550816
	2250504548608 [label=AddBackward0]
	2250504549520 -> 2250504548608
	2250504549520 [label=NativeBatchNormBackward0]
	2250504552208 -> 2250504549520
	2250504552208 [label=ConvolutionBackward0]
	2250504549568 -> 2250504552208
	2250504549568 [label=ReluBackward0]
	2250504548464 -> 2250504549568
	2250504548464 [label=NativeBatchNormBackward0]
	2250504541696 -> 2250504548464
	2250504541696 [label=ConvolutionBackward0]
	2250504541456 -> 2250504541696
	2250504541456 [label=ReluBackward0]
	2250504550000 -> 2250504541456
	2250504550000 [label=NativeBatchNormBackward0]
	2250504550096 -> 2250504550000
	2250504550096 [label=ConvolutionBackward0]
	2250504548656 -> 2250504550096
	2250504548656 [label=ReluBackward0]
	2250504547984 -> 2250504548656
	2250504547984 [label=AddBackward0]
	2250504548128 -> 2250504547984
	2250504548128 [label=NativeBatchNormBackward0]
	2250504547888 -> 2250504548128
	2250504547888 [label=ConvolutionBackward0]
	2250504542800 -> 2250504547888
	2250504542800 [label=ReluBackward0]
	2250504545008 -> 2250504542800
	2250504545008 [label=NativeBatchNormBackward0]
	2250504543280 -> 2250504545008
	2250504543280 [label=ConvolutionBackward0]
	2250504542416 -> 2250504543280
	2250504542416 [label=ReluBackward0]
	2250504542128 -> 2250504542416
	2250504542128 [label=NativeBatchNormBackward0]
	2250504542512 -> 2250504542128
	2250504542512 [label=ConvolutionBackward0]
	2250504548080 -> 2250504542512
	2250504548080 [label=ReluBackward0]
	2250504541504 -> 2250504548080
	2250504541504 [label=AddBackward0]
	2250504540928 -> 2250504541504
	2250504540928 [label=NativeBatchNormBackward0]
	2250504544720 -> 2250504540928
	2250504544720 [label=ConvolutionBackward0]
	2250504536272 -> 2250504544720
	2250504536272 [label=ReluBackward0]
	2250504537184 -> 2250504536272
	2250504537184 [label=NativeBatchNormBackward0]
	2250504540448 -> 2250504537184
	2250504540448 [label=ConvolutionBackward0]
	2250504536368 -> 2250504540448
	2250504536368 [label=ReluBackward0]
	2250504537712 -> 2250504536368
	2250504537712 [label=NativeBatchNormBackward0]
	2250504536320 -> 2250504537712
	2250504536320 [label=ConvolutionBackward0]
	2250504537808 -> 2250504536320
	2250504537808 [label=ReluBackward0]
	2250504537472 -> 2250504537808
	2250504537472 [label=AddBackward0]
	2250504537376 -> 2250504537472
	2250504537376 [label=NativeBatchNormBackward0]
	2250504539968 -> 2250504537376
	2250504539968 [label=ConvolutionBackward0]
	2250504540304 -> 2250504539968
	2250504540304 [label=ReluBackward0]
	2250504539872 -> 2250504540304
	2250504539872 [label=NativeBatchNormBackward0]
	2250504544768 -> 2250504539872
	2250504544768 [label=ConvolutionBackward0]
	2250504536800 -> 2250504544768
	2250504536800 [label=ReluBackward0]
	2250504537136 -> 2250504536800
	2250504537136 [label=NativeBatchNormBackward0]
	2250504536992 -> 2250504537136
	2250504536992 [label=ConvolutionBackward0]
	2250504537616 -> 2250504536992
	2250504537616 [label=ReluBackward0]
	2250504543760 -> 2250504537616
	2250504543760 [label=AddBackward0]
	2250504537040 -> 2250504543760
	2250504537040 [label=NativeBatchNormBackward0]
	2250504608432 -> 2250504537040
	2250504608432 [label=ConvolutionBackward0]
	2250504601760 -> 2250504608432
	2250504601760 [label=ReluBackward0]
	2250504610352 -> 2250504601760
	2250504610352 [label=NativeBatchNormBackward0]
	2250504613760 -> 2250504610352
	2250504613760 [label=ConvolutionBackward0]
	2250504615440 -> 2250504613760
	2250504615440 [label=ReluBackward0]
	2250504605792 -> 2250504615440
	2250504605792 [label=NativeBatchNormBackward0]
	2250504606704 -> 2250504605792
	2250504606704 [label=ConvolutionBackward0]
	2250504536944 -> 2250504606704
	2250504536944 [label=ReluBackward0]
	2250505149200 -> 2250504536944
	2250505149200 [label=AddBackward0]
	2250505146032 -> 2250505149200
	2250505146032 [label=NativeBatchNormBackward0]
	2250504520128 -> 2250505146032
	2250504520128 [label=ConvolutionBackward0]
	2250504534960 -> 2250504520128
	2250504534960 [label=ReluBackward0]
	2250504534672 -> 2250504534960
	2250504534672 [label=NativeBatchNormBackward0]
	2250504527808 -> 2250504534672
	2250504527808 [label=ConvolutionBackward0]
	2250504527472 -> 2250504527808
	2250504527472 [label=ReluBackward0]
	2250504525888 -> 2250504527472
	2250504525888 [label=NativeBatchNormBackward0]
	2250504524688 -> 2250504525888
	2250504524688 [label=ConvolutionBackward0]
	2250504524448 -> 2250504524688
	2250504524448 [label=MaxPool2DWithIndicesBackward0]
	2250504524304 -> 2250504524448
	2250504524304 [label=ReluBackward0]
	2250504524112 -> 2250504524304
	2250504524112 [label=NativeBatchNormBackward0]
	2250504523920 -> 2250504524112
	2250504523920 [label=ConvolutionBackward0]
	2250504523680 -> 2250504523920
	2250450122960 [label="conv1.weight
 (64, 3, 7, 7)" fillcolor=lightblue]
	2250450122960 -> 2250504523680
	2250504523680 [label=AccumulateGrad]
	2250504523872 -> 2250504524112
	2250465798288 [label="bn1.weight
 (64)" fillcolor=lightblue]
	2250465798288 -> 2250504523872
	2250504523872 [label=AccumulateGrad]
	2250504524496 -> 2250504524112
	2250504506560 [label="bn1.bias
 (64)" fillcolor=lightblue]
	2250504506560 -> 2250504524496
	2250504524496 [label=AccumulateGrad]
	2250504524640 -> 2250504524688
	2250461153856 [label="layer1.0.conv1.weight
 (64, 64, 1, 1)" fillcolor=lightblue]
	2250461153856 -> 2250504524640
	2250504524640 [label=AccumulateGrad]
	2250504527088 -> 2250504525888
	2250379287008 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	2250379287008 -> 2250504527088
	2250504527088 [label=AccumulateGrad]
	2250504527232 -> 2250504525888
	2250379286928 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	2250379286928 -> 2250504527232
	2250504527232 [label=AccumulateGrad]
	2250504527424 -> 2250504527808
	2250379286368 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2250379286368 -> 2250504527424
	2250504527424 [label=AccumulateGrad]
	2250504534720 -> 2250504534672
	2250379286048 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	2250379286048 -> 2250504534720
	2250504534720 [label=AccumulateGrad]
	2250504535008 -> 2250504534672
	2250379286208 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	2250379286208 -> 2250504535008
	2250504535008 [label=AccumulateGrad]
	2250504534816 -> 2250504520128
	2250379294528 [label="layer1.0.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2250379294528 -> 2250504534816
	2250504534816 [label=AccumulateGrad]
	2250504520944 -> 2250505146032
	2250379285328 [label="layer1.0.bn3.weight
 (256)" fillcolor=lightblue]
	2250379285328 -> 2250504520944
	2250504520944 [label=AccumulateGrad]
	2250504520704 -> 2250505146032
	2250379285488 [label="layer1.0.bn3.bias
 (256)" fillcolor=lightblue]
	2250379285488 -> 2250504520704
	2250504520704 [label=AccumulateGrad]
	2250504520752 -> 2250505149200
	2250504520752 [label=NativeBatchNormBackward0]
	2250504527664 -> 2250504520752
	2250504527664 [label=ConvolutionBackward0]
	2250504524448 -> 2250504527664
	2250504525792 -> 2250504527664
	2249974714176 [label="layer1.0.downsample.0.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2249974714176 -> 2250504525792
	2250504525792 [label=AccumulateGrad]
	2250504520320 -> 2250504520752
	2250502753792 [label="layer1.0.downsample.1.weight
 (256)" fillcolor=lightblue]
	2250502753792 -> 2250504520320
	2250504520320 [label=AccumulateGrad]
	2250504520368 -> 2250504520752
	2250467712656 [label="layer1.0.downsample.1.bias
 (256)" fillcolor=lightblue]
	2250467712656 -> 2250504520368
	2250504520368 [label=AccumulateGrad]
	2250504612032 -> 2250504606704
	2250379294048 [label="layer1.1.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2250379294048 -> 2250504612032
	2250504612032 [label=AccumulateGrad]
	2250504606656 -> 2250504605792
	2250379284608 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	2250379284608 -> 2250504606656
	2250504606656 [label=AccumulateGrad]
	2250504612608 -> 2250504605792
	2250379284768 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	2250379284768 -> 2250504612608
	2250504612608 [label=AccumulateGrad]
	2250504602144 -> 2250504613760
	2250379283808 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2250379283808 -> 2250504602144
	2250504602144 [label=AccumulateGrad]
	2250504614096 -> 2250504610352
	2250379283728 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	2250379283728 -> 2250504614096
	2250504614096 [label=AccumulateGrad]
	2250504607136 -> 2250504610352
	2250379283568 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	2250379283568 -> 2250504607136
	2250504607136 [label=AccumulateGrad]
	2250504604016 -> 2250504608432
	2250379282288 [label="layer1.1.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2250379282288 -> 2250504604016
	2250504604016 [label=AccumulateGrad]
	2250504609536 -> 2250504537040
	2250379281968 [label="layer1.1.bn3.weight
 (256)" fillcolor=lightblue]
	2250379281968 -> 2250504609536
	2250504609536 [label=AccumulateGrad]
	2250504613712 -> 2250504537040
	2250379282128 [label="layer1.1.bn3.bias
 (256)" fillcolor=lightblue]
	2250379282128 -> 2250504613712
	2250504613712 [label=AccumulateGrad]
	2250504536944 -> 2250504543760
	2250504551440 -> 2250504536992
	2250379281808 [label="layer1.2.conv1.weight
 (64, 256, 1, 1)" fillcolor=lightblue]
	2250379281808 -> 2250504551440
	2250504551440 [label=AccumulateGrad]
	2250504536608 -> 2250504537136
	2250379293248 [label="layer1.2.bn1.weight
 (64)" fillcolor=lightblue]
	2250379293248 -> 2250504536608
	2250504536608 [label=AccumulateGrad]
	2250504537088 -> 2250504537136
	2250379293168 [label="layer1.2.bn1.bias
 (64)" fillcolor=lightblue]
	2250379293168 -> 2250504537088
	2250504537088 [label=AccumulateGrad]
	2250504539728 -> 2250504544768
	2250379289488 [label="layer1.2.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	2250379289488 -> 2250504539728
	2250504539728 [label=AccumulateGrad]
	2250504539680 -> 2250504539872
	2250379330400 [label="layer1.2.bn2.weight
 (64)" fillcolor=lightblue]
	2250379330400 -> 2250504539680
	2250504539680 [label=AccumulateGrad]
	2250504539776 -> 2250504539872
	2250379330320 [label="layer1.2.bn2.bias
 (64)" fillcolor=lightblue]
	2250379330320 -> 2250504539776
	2250504539776 [label=AccumulateGrad]
	2250504540688 -> 2250504539968
	2250379343440 [label="layer1.2.conv3.weight
 (256, 64, 1, 1)" fillcolor=lightblue]
	2250379343440 -> 2250504540688
	2250504540688 [label=AccumulateGrad]
	2250504540016 -> 2250504537376
	2250460850880 [label="layer1.2.bn3.weight
 (256)" fillcolor=lightblue]
	2250460850880 -> 2250504540016
	2250504540016 [label=AccumulateGrad]
	2250504537520 -> 2250504537376
	2250379330080 [label="layer1.2.bn3.bias
 (256)" fillcolor=lightblue]
	2250379330080 -> 2250504537520
	2250504537520 [label=AccumulateGrad]
	2250504537616 -> 2250504537472
	2250504537664 -> 2250504536320
	2250379336320 [label="layer2.0.conv1.weight
 (128, 256, 1, 1)" fillcolor=lightblue]
	2250379336320 -> 2250504537664
	2250504537664 [label=AccumulateGrad]
	2250504536464 -> 2250504537712
	2250379336240 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	2250379336240 -> 2250504536464
	2250504536464 [label=AccumulateGrad]
	2250504536224 -> 2250504537712
	2250379343040 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	2250379343040 -> 2250504536224
	2250504536224 [label=AccumulateGrad]
	2250504540496 -> 2250504540448
	2250379342880 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2250379342880 -> 2250504540496
	2250504540496 [label=AccumulateGrad]
	2250504537328 -> 2250504537184
	2250379342800 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	2250379342800 -> 2250504537328
	2250504537328 [label=AccumulateGrad]
	2250504536176 -> 2250504537184
	2250379336000 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	2250379336000 -> 2250504536176
	2250504536176 [label=AccumulateGrad]
	2250504544672 -> 2250504544720
	2250379342560 [label="layer2.0.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2250379342560 -> 2250504544672
	2250504544672 [label=AccumulateGrad]
	2250504540208 -> 2250504540928
	2250379342480 [label="layer2.0.bn3.weight
 (512)" fillcolor=lightblue]
	2250379342480 -> 2250504540208
	2250504540208 [label=AccumulateGrad]
	2250504540880 -> 2250504540928
	2250379335520 [label="layer2.0.bn3.bias
 (512)" fillcolor=lightblue]
	2250379335520 -> 2250504540880
	2250504540880 [label=AccumulateGrad]
	2250504540256 -> 2250504541504
	2250504540256 [label=NativeBatchNormBackward0]
	2250504540352 -> 2250504540256
	2250504540352 [label=ConvolutionBackward0]
	2250504537808 -> 2250504540352
	2250504537760 -> 2250504540352
	2250379329920 [label="layer2.0.downsample.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	2250379329920 -> 2250504537760
	2250504537760 [label=AccumulateGrad]
	2250504540160 -> 2250504540256
	2250379336480 [label="layer2.0.downsample.1.weight
 (512)" fillcolor=lightblue]
	2250379336480 -> 2250504540160
	2250504540160 [label=AccumulateGrad]
	2250504539824 -> 2250504540256
	2250379336400 [label="layer2.0.downsample.1.bias
 (512)" fillcolor=lightblue]
	2250379336400 -> 2250504539824
	2250504539824 [label=AccumulateGrad]
	2250504540976 -> 2250504542512
	2250379342400 [label="layer2.1.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2250379342400 -> 2250504540976
	2250504540976 [label=AccumulateGrad]
	2250504542464 -> 2250504542128
	2250379342320 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	2250379342320 -> 2250504542464
	2250504542464 [label=AccumulateGrad]
	2250504542848 -> 2250504542128
	2250379335360 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	2250379335360 -> 2250504542848
	2250504542848 [label=AccumulateGrad]
	2250504544576 -> 2250504543280
	2250379341920 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2250379341920 -> 2250504544576
	2250504544576 [label=AccumulateGrad]
	2250504545056 -> 2250504545008
	2250379341840 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	2250379341840 -> 2250504545056
	2250504545056 [label=AccumulateGrad]
	2250504547840 -> 2250504545008
	2250379335040 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	2250379335040 -> 2250504547840
	2250504547840 [label=AccumulateGrad]
	2250504542752 -> 2250504547888
	2250379341600 [label="layer2.1.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2250379341600 -> 2250504542752
	2250504542752 [label=AccumulateGrad]
	2250504542704 -> 2250504548128
	2250379341520 [label="layer2.1.bn3.weight
 (512)" fillcolor=lightblue]
	2250379341520 -> 2250504542704
	2250504542704 [label=AccumulateGrad]
	2250504548176 -> 2250504548128
	2250379334720 [label="layer2.1.bn3.bias
 (512)" fillcolor=lightblue]
	2250379334720 -> 2250504548176
	2250504548176 [label=AccumulateGrad]
	2250504548080 -> 2250504547984
	2250504548224 -> 2250504550096
	2250379341280 [label="layer2.2.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2250379341280 -> 2250504548224
	2250504548224 [label=AccumulateGrad]
	2250504550048 -> 2250504550000
	2250379341200 [label="layer2.2.bn1.weight
 (128)" fillcolor=lightblue]
	2250379341200 -> 2250504550048
	2250504550048 [label=AccumulateGrad]
	2250504536416 -> 2250504550000
	2250379334400 [label="layer2.2.bn1.bias
 (128)" fillcolor=lightblue]
	2250379334400 -> 2250504536416
	2250504536416 [label=AccumulateGrad]
	2250504541552 -> 2250504541696
	2250379340960 [label="layer2.2.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2250379340960 -> 2250504541552
	2250504541552 [label=AccumulateGrad]
	2250504541648 -> 2250504548464
	2250379334080 [label="layer2.2.bn2.weight
 (128)" fillcolor=lightblue]
	2250379334080 -> 2250504541648
	2250504541648 [label=AccumulateGrad]
	2250504548368 -> 2250504548464
	2250379334000 [label="layer2.2.bn2.bias
 (128)" fillcolor=lightblue]
	2250379334000 -> 2250504548368
	2250504548368 [label=AccumulateGrad]
	2250504549472 -> 2250504552208
	2250379333680 [label="layer2.2.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2250379333680 -> 2250504549472
	2250504549472 [label=AccumulateGrad]
	2250504552256 -> 2250504549520
	2250379333760 [label="layer2.2.bn3.weight
 (512)" fillcolor=lightblue]
	2250379333760 -> 2250504552256
	2250504552256 [label=AccumulateGrad]
	2250504552304 -> 2250504549520
	2250379340720 [label="layer2.2.bn3.bias
 (512)" fillcolor=lightblue]
	2250379340720 -> 2250504552304
	2250504552304 [label=AccumulateGrad]
	2250504548656 -> 2250504548608
	2250504549184 -> 2250504548944
	2250379333280 [label="layer2.3.conv1.weight
 (128, 512, 1, 1)" fillcolor=lightblue]
	2250379333280 -> 2250504549184
	2250504549184 [label=AccumulateGrad]
	2250504548896 -> 2250504552064
	2250379333200 [label="layer2.3.bn1.weight
 (128)" fillcolor=lightblue]
	2250379333200 -> 2250504548896
	2250504548896 [label=AccumulateGrad]
	2250504549328 -> 2250504552064
	2250379340240 [label="layer2.3.bn1.bias
 (128)" fillcolor=lightblue]
	2250379340240 -> 2250504549328
	2250504549328 [label=AccumulateGrad]
	2250504549280 -> 2250504549136
	2250379333120 [label="layer2.3.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	2250379333120 -> 2250504549280
	2250504549280 [label=AccumulateGrad]
	2250504551968 -> 2250504549040
	2250379340320 [label="layer2.3.bn2.weight
 (128)" fillcolor=lightblue]
	2250379340320 -> 2250504551968
	2250504551968 [label=AccumulateGrad]
	2250504542080 -> 2250504549040
	2250379333040 [label="layer2.3.bn2.bias
 (128)" fillcolor=lightblue]
	2250379333040 -> 2250504542080
	2250504542080 [label=AccumulateGrad]
	2250504549760 -> 2250504550864
	2250379339840 [label="layer2.3.conv3.weight
 (512, 128, 1, 1)" fillcolor=lightblue]
	2250379339840 -> 2250504549760
	2250504549760 [label=AccumulateGrad]
	2250504550768 -> 2250504549808
	2250379332800 [label="layer2.3.bn3.weight
 (512)" fillcolor=lightblue]
	2250379332800 -> 2250504550768
	2250504550768 [label=AccumulateGrad]
	2250504549856 -> 2250504549808
	2250379332720 [label="layer2.3.bn3.bias
 (512)" fillcolor=lightblue]
	2250379332720 -> 2250504549856
	2250504549856 [label=AccumulateGrad]
	2250504550816 -> 2250504549952
	2250504548800 -> 2250504550240
	2250379332160 [label="layer3.0.conv1.weight
 (256, 512, 1, 1)" fillcolor=lightblue]
	2250379332160 -> 2250504548800
	2250504548800 [label=AccumulateGrad]
	2250504550144 -> 2250504550624
	2250379332080 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	2250379332080 -> 2250504550144
	2250504550144 [label=AccumulateGrad]
	2250504550576 -> 2250504550624
	2250379339120 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	2250379339120 -> 2250504550576
	2250504550576 [label=AccumulateGrad]
	2250504550384 -> 2250504550336
	2250379331840 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250379331840 -> 2250504550384
	2250504550384 [label=AccumulateGrad]
	2250504550672 -> 2250504542032
	2250379331760 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	2250379331760 -> 2250504550672
	2250504550672 [label=AccumulateGrad]
	2250504549616 -> 2250504542032
	2250379338800 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	2250379338800 -> 2250504549616
	2250504549616 [label=AccumulateGrad]
	2250504541936 -> 2250504480816
	2250379338560 [label="layer3.0.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250379338560 -> 2250504541936
	2250504541936 [label=AccumulateGrad]
	2250504479280 -> 2250504479904
	2250379331520 [label="layer3.0.bn3.weight
 (1024)" fillcolor=lightblue]
	2250379331520 -> 2250504479280
	2250504479280 [label=AccumulateGrad]
	2250504540736 -> 2250504479904
	2250379338480 [label="layer3.0.bn3.bias
 (1024)" fillcolor=lightblue]
	2250379338480 -> 2250504540736
	2250504540736 [label=AccumulateGrad]
	2250504479664 -> 2250504479808
	2250504479664 [label=NativeBatchNormBackward0]
	2250504550432 -> 2250504479664
	2250504550432 [label=ConvolutionBackward0]
	2250504550720 -> 2250504550432
	2250504550192 -> 2250504550432
	2250379332560 [label="layer3.0.downsample.0.weight
 (1024, 512, 1, 1)" fillcolor=lightblue]
	2250379332560 -> 2250504550192
	2250504550192 [label=AccumulateGrad]
	2250504541744 -> 2250504479664
	2250379339600 [label="layer3.0.downsample.1.weight
 (1024)" fillcolor=lightblue]
	2250379339600 -> 2250504541744
	2250504541744 [label=AccumulateGrad]
	2250504541840 -> 2250504479664
	2250379339520 [label="layer3.0.downsample.1.bias
 (1024)" fillcolor=lightblue]
	2250379339520 -> 2250504541840
	2250504541840 [label=AccumulateGrad]
	2250504479760 -> 2250504481344
	2250379330560 [label="layer3.1.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2250379330560 -> 2250504479760
	2250504479760 [label=AccumulateGrad]
	2250504482928 -> 2250504470880
	2250379330480 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	2250379330480 -> 2250504482928
	2250504482928 [label=AccumulateGrad]
	2250504486864 -> 2250504470880
	2250379336960 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	2250379336960 -> 2250504486864
	2250504486864 [label=AccumulateGrad]
	2250504481440 -> 2250504480288
	2250505682784 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250505682784 -> 2250504481440
	2250504481440 [label=AccumulateGrad]
	2250504475104 -> 2250504480048
	2250505682864 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	2250505682864 -> 2250504475104
	2250504475104 [label=AccumulateGrad]
	2250504481536 -> 2250504480048
	2250506600512 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	2250506600512 -> 2250504481536
	2250504481536 [label=AccumulateGrad]
	2250504481632 -> 2250504475392
	2250506600992 [label="layer3.1.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250506600992 -> 2250504481632
	2250504481632 [label=AccumulateGrad]
	2250504475872 -> 2250504481680
	2250506601072 [label="layer3.1.bn3.weight
 (1024)" fillcolor=lightblue]
	2250506601072 -> 2250504475872
	2250504475872 [label=AccumulateGrad]
	2250504479472 -> 2250504481680
	2250506601152 [label="layer3.1.bn3.bias
 (1024)" fillcolor=lightblue]
	2250506601152 -> 2250504479472
	2250504479472 [label=AccumulateGrad]
	2250504480336 -> 2250504485520
	2250504475632 -> 2250504480480
	2250506601552 [label="layer3.2.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2250506601552 -> 2250504475632
	2250504475632 [label=AccumulateGrad]
	2250504476544 -> 2250504476016
	2250506601632 [label="layer3.2.bn1.weight
 (256)" fillcolor=lightblue]
	2250506601632 -> 2250504476544
	2250504476544 [label=AccumulateGrad]
	2250504483024 -> 2250504476016
	2250506601712 [label="layer3.2.bn1.bias
 (256)" fillcolor=lightblue]
	2250506601712 -> 2250504483024
	2250504483024 [label=AccumulateGrad]
	2250504486576 -> 2250504480096
	2250506602192 [label="layer3.2.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250506602192 -> 2250504486576
	2250504486576 [label=AccumulateGrad]
	2250504486144 -> 2250504480528
	2250506602272 [label="layer3.2.bn2.weight
 (256)" fillcolor=lightblue]
	2250506602272 -> 2250504486144
	2250504486144 [label=AccumulateGrad]
	2250504481296 -> 2250504480528
	2250506602352 [label="layer3.2.bn2.bias
 (256)" fillcolor=lightblue]
	2250506602352 -> 2250504481296
	2250504481296 [label=AccumulateGrad]
	2250504482352 -> 2250504414464
	2250506602832 [label="layer3.2.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250506602832 -> 2250504482352
	2250504482352 [label=AccumulateGrad]
	2250504414320 -> 2250504414224
	2250506602912 [label="layer3.2.bn3.weight
 (1024)" fillcolor=lightblue]
	2250506602912 -> 2250504414320
	2250504414320 [label=AccumulateGrad]
	2250504414368 -> 2250504414224
	2250506602992 [label="layer3.2.bn3.bias
 (1024)" fillcolor=lightblue]
	2250506602992 -> 2250504414368
	2250504414368 [label=AccumulateGrad]
	2250504414272 -> 2250504405296
	2250504408992 -> 2250504419456
	2250506603392 [label="layer3.3.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2250506603392 -> 2250504408992
	2250504408992 [label=AccumulateGrad]
	2250504408656 -> 2250504408704
	2250506603472 [label="layer3.3.bn1.weight
 (256)" fillcolor=lightblue]
	2250506603472 -> 2250504408656
	2250504408656 [label=AccumulateGrad]
	2250504405440 -> 2250504408704
	2250506603552 [label="layer3.3.bn1.bias
 (256)" fillcolor=lightblue]
	2250506603552 -> 2250504405440
	2250504405440 [label=AccumulateGrad]
	2250504419024 -> 2250504408752
	2250506604032 [label="layer3.3.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250506604032 -> 2250504419024
	2250504419024 [label=AccumulateGrad]
	2250504419408 -> 2250504405488
	2250506604112 [label="layer3.3.bn2.weight
 (256)" fillcolor=lightblue]
	2250506604112 -> 2250504419408
	2250504419408 [label=AccumulateGrad]
	2250504413360 -> 2250504405488
	2250506604192 [label="layer3.3.bn2.bias
 (256)" fillcolor=lightblue]
	2250506604192 -> 2250504413360
	2250504413360 [label=AccumulateGrad]
	2250504419072 -> 2250504410240
	2250506604672 [label="layer3.3.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250506604672 -> 2250504419072
	2250504419072 [label=AccumulateGrad]
	2250504410336 -> 2250504410384
	2250506604592 [label="layer3.3.bn3.weight
 (1024)" fillcolor=lightblue]
	2250506604592 -> 2250504410336
	2250504410336 [label=AccumulateGrad]
	2250504410288 -> 2250504410384
	2250506604752 [label="layer3.3.bn3.bias
 (1024)" fillcolor=lightblue]
	2250506604752 -> 2250504410288
	2250504410288 [label=AccumulateGrad]
	2250504413312 -> 2250504413264
	2250504413600 -> 2250504409280
	2250506605232 [label="layer3.4.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2250506605232 -> 2250504413600
	2250504413600 [label=AccumulateGrad]
	2250504410432 -> 2250504415520
	2250506605312 [label="layer3.4.bn1.weight
 (256)" fillcolor=lightblue]
	2250506605312 -> 2250504410432
	2250504410432 [label=AccumulateGrad]
	2250504414896 -> 2250504415520
	2250506605392 [label="layer3.4.bn1.bias
 (256)" fillcolor=lightblue]
	2250506605392 -> 2250504414896
	2250504414896 [label=AccumulateGrad]
	2250504418496 -> 2250504418400
	2250506605872 [label="layer3.4.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250506605872 -> 2250504418496
	2250504418496 [label=AccumulateGrad]
	2250504418304 -> 2250504418256
	2250506605952 [label="layer3.4.bn2.weight
 (256)" fillcolor=lightblue]
	2250506605952 -> 2250504418304
	2250504418304 [label=AccumulateGrad]
	2250504413936 -> 2250504418256
	2250506606032 [label="layer3.4.bn2.bias
 (256)" fillcolor=lightblue]
	2250506606032 -> 2250504413936
	2250504413936 [label=AccumulateGrad]
	2250504414032 -> 2250504413840
	2250506606432 [label="layer3.4.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250506606432 -> 2250504414032
	2250504414032 [label=AccumulateGrad]
	2250504413888 -> 2250504413456
	2250506606512 [label="layer3.4.bn3.weight
 (1024)" fillcolor=lightblue]
	2250506606512 -> 2250504413888
	2250504413888 [label=AccumulateGrad]
	2250504413408 -> 2250504413456
	2250506606592 [label="layer3.4.bn3.bias
 (1024)" fillcolor=lightblue]
	2250506606592 -> 2250504413408
	2250504413408 [label=AccumulateGrad]
	2250504413504 -> 2250504413552
	2250504418112 -> 2250504414080
	2250506607072 [label="layer3.5.conv1.weight
 (256, 1024, 1, 1)" fillcolor=lightblue]
	2250506607072 -> 2250504418112
	2250504418112 [label=AccumulateGrad]
	2250504415136 -> 2250504407456
	2250506607152 [label="layer3.5.bn1.weight
 (256)" fillcolor=lightblue]
	2250506607152 -> 2250504415136
	2250504415136 [label=AccumulateGrad]
	2250504407216 -> 2250504407456
	2250506607232 [label="layer3.5.bn1.bias
 (256)" fillcolor=lightblue]
	2250506607232 -> 2250504407216
	2250504407216 [label=AccumulateGrad]
	2250504415472 -> 2250504417392
	2250506607712 [label="layer3.5.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	2250506607712 -> 2250504415472
	2250504415472 [label=AccumulateGrad]
	2250504411728 -> 2250504411824
	2250506607792 [label="layer3.5.bn2.weight
 (256)" fillcolor=lightblue]
	2250506607792 -> 2250504411728
	2250504411728 [label=AccumulateGrad]
	2250504416480 -> 2250504411824
	2250506607872 [label="layer3.5.bn2.bias
 (256)" fillcolor=lightblue]
	2250506607872 -> 2250504416480
	2250504416480 [label=AccumulateGrad]
	2250504410048 -> 2250504407984
	2250506608352 [label="layer3.5.conv3.weight
 (1024, 256, 1, 1)" fillcolor=lightblue]
	2250506608352 -> 2250504410048
	2250504410048 [label=AccumulateGrad]
	2250504407936 -> 2250504414608
	2250506608432 [label="layer3.5.bn3.weight
 (1024)" fillcolor=lightblue]
	2250506608432 -> 2250504407936
	2250504407936 [label=AccumulateGrad]
	2250504414800 -> 2250504414608
	2250506608512 [label="layer3.5.bn3.bias
 (1024)" fillcolor=lightblue]
	2250506608512 -> 2250504414800
	2250504414800 [label=AccumulateGrad]
	2250504417344 -> 2250504413216
	2250504406880 -> 2250504416720
	2250506609632 [label="layer4.0.conv1.weight
 (512, 1024, 1, 1)" fillcolor=lightblue]
	2250506609632 -> 2250504406880
	2250504406880 [label=AccumulateGrad]
	2250504417920 -> 2250504418016
	2250506609712 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	2250506609712 -> 2250504417920
	2250504417920 [label=AccumulateGrad]
	2250504410528 -> 2250504418016
	2250506609792 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	2250506609792 -> 2250504410528
	2250504410528 [label=AccumulateGrad]
	2250504407744 -> 2250504408128
	2250506610272 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2250506610272 -> 2250504407744
	2250504407744 [label=AccumulateGrad]
	2250504405776 -> 2250504419984
	2250506610352 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	2250506610352 -> 2250504405776
	2250504405776 [label=AccumulateGrad]
	2250504415664 -> 2250504419984
	2250506610432 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	2250506610432 -> 2250504415664
	2250504415664 [label=AccumulateGrad]
	2250504407600 -> 2250504413168
	2250506610832 [label="layer4.0.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2250506610832 -> 2250504407600
	2250504407600 [label=AccumulateGrad]
	2250504418736 -> 2250504405248
	2250506610912 [label="layer4.0.bn3.weight
 (2048)" fillcolor=lightblue]
	2250506610912 -> 2250504418736
	2250504418736 [label=AccumulateGrad]
	2250504409088 -> 2250504405248
	2250506610992 [label="layer4.0.bn3.bias
 (2048)" fillcolor=lightblue]
	2250506610992 -> 2250504409088
	2250504409088 [label=AccumulateGrad]
	2250504416048 -> 2250504409520
	2250504416048 [label=NativeBatchNormBackward0]
	2250504417200 -> 2250504416048
	2250504417200 [label=ConvolutionBackward0]
	2250504405200 -> 2250504417200
	2250504417728 -> 2250504417200
	2250506608992 [label="layer4.0.downsample.0.weight
 (2048, 1024, 1, 1)" fillcolor=lightblue]
	2250506608992 -> 2250504417728
	2250504417728 [label=AccumulateGrad]
	2250504411248 -> 2250504416048
	2250506609072 [label="layer4.0.downsample.1.weight
 (2048)" fillcolor=lightblue]
	2250506609072 -> 2250504411248
	2250504411248 [label=AccumulateGrad]
	2250504412496 -> 2250504416048
	2250506609152 [label="layer4.0.downsample.1.bias
 (2048)" fillcolor=lightblue]
	2250506609152 -> 2250504412496
	2250504412496 [label=AccumulateGrad]
	2250504420944 -> 2250504412352
	2250506611472 [label="layer4.1.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2250506611472 -> 2250504420944
	2250504420944 [label=AccumulateGrad]
	2250504412640 -> 2250504412304
	2250506611552 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	2250506611552 -> 2250504412640
	2250504412640 [label=AccumulateGrad]
	2250504415568 -> 2250504412304
	2250506611632 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	2250506611632 -> 2250504415568
	2250504415568 [label=AccumulateGrad]
	2250504419504 -> 2250504411296
	2250506612112 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2250506612112 -> 2250504419504
	2250504419504 [label=AccumulateGrad]
	2250504407888 -> 2250504420800
	2250506612192 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	2250506612192 -> 2250504407888
	2250504407888 [label=AccumulateGrad]
	2250504419552 -> 2250504420800
	2250506612272 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	2250506612272 -> 2250504419552
	2250504419552 [label=AccumulateGrad]
	2250504414656 -> 2250504409472
	2250506612672 [label="layer4.1.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2250506612672 -> 2250504414656
	2250504414656 [label=AccumulateGrad]
	2250504417680 -> 2250504415952
	2250506612752 [label="layer4.1.bn3.weight
 (2048)" fillcolor=lightblue]
	2250506612752 -> 2250504417680
	2250504417680 [label=AccumulateGrad]
	2250504418976 -> 2250504415952
	2250506612832 [label="layer4.1.bn3.bias
 (2048)" fillcolor=lightblue]
	2250506612832 -> 2250504418976
	2250504418976 [label=AccumulateGrad]
	2250504415616 -> 2250504416624
	2250504412448 -> 2250504419312
	2250506613152 [label="layer4.2.conv1.weight
 (512, 2048, 1, 1)" fillcolor=lightblue]
	2250506613152 -> 2250504412448
	2250504412448 [label=AccumulateGrad]
	2250504410480 -> 2250504414560
	2250506613312 [label="layer4.2.bn1.weight
 (512)" fillcolor=lightblue]
	2250506613312 -> 2250504410480
	2250504410480 [label=AccumulateGrad]
	2250504418064 -> 2250504414560
	2250506613392 [label="layer4.2.bn1.bias
 (512)" fillcolor=lightblue]
	2250506613392 -> 2250504418064
	2250504418064 [label=AccumulateGrad]
	2250504416960 -> 2250504418592
	2250506613872 [label="layer4.2.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	2250506613872 -> 2250504416960
	2250504416960 [label=AccumulateGrad]
	2250504408608 -> 2250504417872
	2250506613952 [label="layer4.2.bn2.weight
 (512)" fillcolor=lightblue]
	2250506613952 -> 2250504408608
	2250504408608 [label=AccumulateGrad]
	2250504408560 -> 2250504417872
	2250506614032 [label="layer4.2.bn2.bias
 (512)" fillcolor=lightblue]
	2250506614032 -> 2250504408560
	2250504408560 [label=AccumulateGrad]
	2250504416192 -> 2250504408464
	2250506614512 [label="layer4.2.conv3.weight
 (2048, 512, 1, 1)" fillcolor=lightblue]
	2250506614512 -> 2250504416192
	2250504416192 [label=AccumulateGrad]
	2250504405968 -> 2250504409376
	2250506614592 [label="layer4.2.bn3.weight
 (2048)" fillcolor=lightblue]
	2250506614592 -> 2250504405968
	2250504405968 [label=AccumulateGrad]
	2250504408416 -> 2250504409376
	2250506614672 [label="layer4.2.bn3.bias
 (2048)" fillcolor=lightblue]
	2250506614672 -> 2250504408416
	2250504408416 [label=AccumulateGrad]
	2250504410000 -> 2250504406736
	2250504412016 -> 2250504406592
	2250504412016 [label=TBackward0]
	2250504409184 -> 2250504412016
	2250506615072 [label="fc.weight
 (10, 2048)" fillcolor=lightblue]
	2250506615072 -> 2250504409184
	2250504409184 [label=AccumulateGrad]
	2250504406592 -> 2248759602016
}
